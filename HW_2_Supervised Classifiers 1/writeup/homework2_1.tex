\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{enumerate}
\usepackage{amsmath,bm}
\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm




\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}



\pagestyle{myheadings} 
\markboth{Homework 2}{Fall 2015 CS 475 Machine Learning: Homework 2} 


\title{CS 475 Machine Learning: Homework 2\\Supervised Classifiers 1,\\
Probability, Linear Algebra and Decision Trees\\
\Large{Due: Tuesday September 29, 2015, 11:59pm}\\
100 Points Total \hspace{1cm} Version 1.1}

\author{}
\date{}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\vspace{-.5in}

{\bf Make sure to read from start to finish before beginning the assignment.}
\section{Programming (50 points)}
In this assignment you will write a logistic regression classifier. Your implementation will be very similar to
the algorithm we covered in class. Your code needs to handle data with binary and continuous features and only binary labels (no multi-class).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Logistic Regression}
The logistic regression model is used to model binary classification data. Logistic regression is a special case of generalized linear regression where the labels $Y$ are modeled as a linear combination of the data $X$, but in a transformed space specified by $g$, sometimes called the ``link function":
\begin{equation}
E[y \mid \vx] = g(\vw^T\vx).
\end{equation}
\\
This ``link function" allows you to model inherently non-linear data with a linear model. In the case of logistic regression, the link function is the logistic function:
\begin{equation}
g(z) = \frac{1}{1 + e^{-z}}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stochastic Gradient Descent}
In this assignment, we will solve for the parameters $\vw$ in our logistic regression model using stochastic gradient descent to find the maximum likelihood estimate.

Stochastic gradient descent (SGD) is an optimization technique that is both very simple and powerful. Regular gradient descent works by taking the gradient of the objective function and taking steps in directions where the gradient is negative, which decreases the objective function\footnote{Gradient descent decreases the objective function if the gradient (first order approximation) is locally accurate.}. In stochastic gradient descent instead of exactly finding the gradient of the objective function, which is the expectation of the gradient over all of your training examples, an estimate is taken by sampling from the data. Evaluating the gradient function at a sample of the data produces a random value (the ``stochastic" part of SGD) that has the same expectation as the true gradient, but higher variance. It turns out that using this stochastic gradient instead of the true gradient can often speed up optimization. In this assignment we will sample our data by only using one training instance, $(y, \vx)$, as is most common in SGD.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maximum Conditional Likelihood}
Since we seek to maximize the objective, we will use stochastic gradient \emph{ascent}. We begin by writing the conditional likelihood:
\begin{equation}
P(Y \mid \vw, X) = \prod_{i=1}^n{p(\yi \mid \vw, \vxi)}
\end{equation}

Since $\yi \in \{ 0,1 \}$, we can write the conditional probability inside the product as:
\begin{equation}
P(Y \mid \vw, X) = \prod_{i=1}^n{ p(\yi=1 \mid \vw, \vxi)^{\yi} \times (p(\yi=0 \mid \vw, \vxi))^{1-\yi}}
\end{equation}
Note that one of these terms in the product will have an exponent of 0, and will evaluate to 1.\\

For ease of math and computation, we will take the log:
\begin{equation}
\ell(Y,X,\vw) = \log P(Y \mid \vw, X) = \sum_{i=1}^n{ \yi \log(p(\yi=1 \mid \vw, \vxi)) + (1-\yi)\log(p(\yi=0 \mid \vw, \vxi))}
\end{equation}

Plug in our logistic function for the probability that $y$ is 1:
\begin{equation}
\ell(Y,X,\vw) = \sum_{i=1}^n{ y_i\ \log( g(\vw \vxi) ) + (1-y_i)\ \log( 1-g(\vw \vxi) ))}
\end{equation}

Recall that the link function, $g$, is the logistic function. It has the nice property $1 - g(z) = g(-z)$.
\begin{equation}
\ell(Y,X,\vw) = \sum_{i=1}^n{ y_i\ \log( g(\vw \vxi) ) + (1-y_i)\ \log( g(-\vw \vxi) )}
\end{equation}

We can now use the chain rule to take the gradient with respect to $\vw$:
\begin{equation}
\nabla \ell(Y,X,\vw) = \sum_{i=1}^n{
	y_i\ \frac{1}{g(\vw \vxi)} \nabla g(\vw \vxi)
	+ (1-y_i)\ \frac{1}{g(-\vw \vxi)} \nabla g(-\vw \vxi)
}
\end{equation}

Since $\frac{\partial}{\partial z}g(z) = g(z)(1-g(z))$:
\begin{eqnarray}
\nabla \ell(Y,X,\vw) &=& \sum_{i=1}^n
	 y_i\ \frac{1}{g(\vw \vxi)} g(\vw \vxi) (1-g(\vw \vxi)) \nabla \vw \vxi \\
& & 	+ (1-y_i)\ \frac{1}{g(-\vw \vxi)} g(-\vw \vxi) (1-g(-\vw \vxi)) \nabla (-\vw \vxi)
\end{eqnarray}

Simplify again using $1-g(z) = g(-z)$ and cancel terms
\begin{equation}
\nabla \ell(Y,X,\vw) = \sum_{i=1}^n{
	y_i g(-\vw \vxi) \nabla \vw \vxi
	+ (1-y_i) g(\vw \vxi) \nabla (-\vw \vxi)
}
\end{equation}

You can now get the partial derivatives (components of the gradient) out of this gradient function by:
\begin{equation}
\frac{\partial}{\partial \vw_j} \ell(Y,X,\vw) = \sum_{i=1}^n{
	y_i g(-\vw \vxi) \ \vxij
	+ (1-y_i) g(\vw \vxi) (-\vxij)
}
\end{equation}

Remember that because we are doing stochastic gradient descent, we are not calculating the full gradient, which is the sum over all the data. We are sampling \emph{one training instance} ($y_i, \vx_i$) and evaluating the gradient there. So for each SGD step, the partial derivatives for each weight $\vw_j$ at one point are:
\begin{equation}
\nabla \ell(y_i, \vw, \vxi) = \frac{\partial}{\partial \vw_j} \ell(y_i, \vw, \vxi) =
	y_i g(-\vw \vxi) \vxij + (1-y_i) g(\vw \vxi) (-\vxij)
\end{equation}

It is this equation that you will use to calculate your updates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{AdaGrad: Adaptively Choosing the Learning Rate}
In SGD, the update rule is $\vw' = \vw + \eta \nabla \ell(y_i, \vw, \vxi)$. Choosing $\eta$ intelligently is a non-trivial task, and there are many ways to choose it in the literature. In this assignment, we will use an adaptive gradient method called \textit{AdaGrad}\footnote{http://www.magicbroom.info/Papers/DuchiHaSi10.pdf}, which compute a new $\eta$ for each iteration based on historical  information, so that frequently occurring features in the gradients get smaller learning rates and infrequent features get higher ones. 

AdaGrad provides a per-feature learning rate $\eta_{i,j}$ at each time step $i$ for feature $j$ as:
\begin{align}
\eta_{i,j}=\frac{\eta_0}{\sqrt{I_j+\sum_{t=1}^i f_{t,j}^2(\vw_t)}}
\end{align}
where $f_{t,j}$ is the partial gradient of the objective function at time $t$ for feature $j$ (i.e., $f_{t,j}(\vw)=\frac{\partial}{\partial \vw_j} \ell(y_t, \vw, \vx_t)$). $0 \le I_j \le 1$ is the initial value for the denominator of $\eta_{i,j}$. A larger $I_j$ will prevent $\eta_{1,j}$ from being too large if $f_{i,j}$ is too small. $\eta_0>0$ is a constant scalar.
You can tune $\eta_0$ and $I_j$ for better performance in terms of convergence rate. 

As a result, the update rule for AdaGrad is $\vw_j' = \vw_j + \eta_{i,j} \nabla \ell(y_i, \vw, \vxi)$.

In this assignment, we always \emph{set} $I_j=1$ for all $j$. We also use $\eta_0$. By default it should be 0.01, but your code should allow the user to change it by setting a command line argument called \textbf{sgd\_eta0}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Offset Feature}
None of the math above mentions an offset feature (bias feature), a $\vw_0$, that corresponds to a $x_{i,0}$ that is always 1. It turns out that we don't need this if our data is centered. By centered we mean that $E[y] = 0$. While this may or may not be true, for this assignment you should assume that your data is centered. Do not include another feature that is always 1 ($x_0$) or weight ($\vw_0$) for it.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergence}
In real SGD, you must decide when you have converged. Ideally, a maximized function has a gradient value of 0, but due to issues related to your step size, random noise, and machine precision, your gradient will likely never be exactly zero. Usually people check that the $L_p$ norm of the gradient is less than some $\delta$, for some $p$. For the sake of simplicity and consistent results, we will not do this in this assignment. Instead, your program should take a parameter \textbf{sgd\_iterations} which is \emph{exactly} how many iterations you should run (not an upper bound). An iteration is a single pass over every training example.
The default of \textbf{sgd\_iterations} should be 20.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation Notes}
\begin{enumerate}[1.]
\item Many descriptions of SGD call for shuffling your data before learning. This is a good idea to break any dependence in the order of your data. In order to achieve consistent results for everyone, we are requiring that you \textbf{do not shuffle your data}. When you are training, you will go through your data in the order it appeared in the data file. If you require more iterations than there are lines in the file, then once you hit the last example you should loop around to the first example.

\item
Even though logistic regression predicts a probability that the label is 1, the output of your program should be binary. Round your solution based on whether the probability is greater than or equal to 0.5:\\
\\
$\hat{y}_{new} = 1$ if $p(y = 1 | \vw, \vx) = g(\vw \vx) = \frac{1}{1 + e^{-\vw \vx}} \ge 0.5$\\
$\hat{y}_{new} = 0$ otherwise\\

\item
Initialize the parameters $\vw$ to $0$.

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{How Your Code Will Be Called}

To train a model we will call:
\begin{verbatim}
java cs475.Classify -mode train -algorithm logistic_regression \
        -model_file speech.logistic_regression.model \
        -data speech.train
\end{verbatim}

There are some additional parameters which your program must support during training:
\begin{verbatim}
-sgd_eta0 k	        // sets the the constant scalar, default = 1.0
-sgd_iterations t  // sets the number of SGD iterations, default = 20
\end{verbatim}

All of these parameters are \emph{optional}. If they are not present, they should be set to their default values.\\
\\
To make predictions using a model we will call:
\begin{verbatim}
java cs475.Classify -mode test -algorithm logistic_regression \
        -model_file speech.logistic_regression.model \
        -data speech.test \
        -predictions_file speech.test.predictions
\end{verbatim}

Remember that your output should be 0/1 valued, not real valued.

You can add a command line parameter by adding the following
code block to the {\tt createCommandLineOptions} method of {\tt Classify}.
\begin{footnotesize}
\begin{verbatim}
registerOption("sgd_eta0", "double", true, "The constant scalar for learning rate in AdaGrad.");
registerOption("sgd_iterations", "int", true, "The number of SGD iterations.");
\end{verbatim}
\end{footnotesize}

Be sure to add the option name exactly as it appears above. A common mistake is to change underscores to dashes.

You can read the value from the command line by adding the following to the main method of {\tt Classify}:
\begin{footnotesize}
\begin{verbatim}
int sgd_iterations = 20;
if (CommandLineUtilities.hasArg("sgd_iterations"))
    sgd_iterations = CommandLineUtilities.getOptionValueAsInt("sgd_iterations");
double sgd_eta0 = 1.0;
if (CommandLineUtilities.hasArg("sgd_eta0"))
    sgd_eta0 = CommandLineUtilities.getOptionValueAsFloat("sgd_eta0");
\end{verbatim}

\end{footnotesize}



\section{Analytical (50 points)}

\paragraph{1) Fisher Linear Discriminant and Logistic Regression Classifiers (15 points)}
Generative models and discriminative models are somehow connected given certain scenarios. Suppose that we have samples from two classes with equal prior. The first class of samples have their features independent generated from a multivariate normal distribution $N(\mu_1,\Sigma)$, and the second class of samples have their features independently generated from a multivariate normal distribution $N(\mu_2,\Sigma)$.
\begin{enumerate}[(a)]
\item Prove that the class label $y$ conditioning on the feature vector $X$ follows a logistic regression model.
\item Prove that the classifier based on the logistic regression model obtained in (a) is equivalent the optimal Fisher linear discriminant classifier.  The optimal Fisher linear discriminant classifier is obtained using the population means and covariance matrix; see section 4.1.4 in Bishop.

{\bf Hint:} You only need to show that both classifiers use the same decision rule.
\end{enumerate}


\paragraph{2) Linear Models (10 points)}

Besides the least square estimators, machine learning researchers are also interested in another type of estimators -- maximum likelihood estimators. Consider a linear model $y=X\beta+\epsilon$, where $X\in\mathbb{R}^{n \times d}$ is the design matrix, $y\in\mathbb{R}^{n}$ is the response vector, and $\epsilon\in\mathbb{R}^n$ is the random noise with each entry independently sampled from $N(0,\sigma^2)$. 
%\begin{enumerate}[(a)]
%\item
Please derive the maximum likelihood estimator of $\beta$ and $\sigma$.
%\item Are the obtained MLE estimators in (a) unbiased?

%{\bf Hint:} $P=X(X^TX)^{-1}X^T$ is a rank $d$ projection matrix. Therefore there exists a $U\in\mathbb{R}^{n\times (n-d)}$ such that $U^TU=I$ and $I-P=UU^T$.
%\end{enumerate}


\paragraph{3) Regularization and Overfitting. (5 points)}
Statisticians love linear models because these models are very simple and interpretable. Many variants of linear models has been proposed, and most of them are formulated as (penalized) least squares program. Here we have three least squares programs,
\begin{align}
\hat{\beta}_0 = &\mathop{\rm argmin}_{\beta_0}  \|y-X_1\beta_0\|_2^2, \label{partial}\\
(\hat{\beta}_1,\hat{\beta}_2) = &\mathop{\rm argmin}_{\beta_1,\beta_2}  \|y-X_1\beta_1-X_2\beta_2\|_2^2, \label{complete}\\
\hat{\beta}_3 = &\mathop{\rm argmin}_{\beta_3} \|y-X_1\beta_3\|_2^2 +\lambda\|\beta_3\|_2^2 \label{ridge},
\end{align}
where $\lambda>0$, $y\in\mathbb{R}^n$, $X_1\in\mathbb{R}^{n \times d_1}$, and $X_2\in\mathbb{R}^{n \times d_1}$. \eqref{ridge} is well known as the ridge regression. The square norm acts as a penalty function to reduce overfitting. Prove
\begin{align}\label{overfitting}
\|y-X_1\hat{\beta}_3\|_2^2 \geq \|y-X_1\hat{\beta}_0\|_2^2 \geq \|y-X_1\hat{\beta}_1-X_2\hat{\beta}_2\|_2^2.
\end{align}

\paragraph{4) Decision Tree (10 points)} Let's investigate how accurately decisions trees can learn. We start by constructing a unit square ($[0; 1] \times [0;1]$). We select $n$ samples from the square, each with a binary label ($+1$ or $-1$), such that no two samples share either $x$ or $y$ coordinates.  Unlike the programming above, each feature can be used multiple times in a decision tree. At each node we can only conduct a binary threshold split using one single feature. 
\begin{enumerate}[(a)]
\item Prove that we can find a decision tree of depth at most $\log_2n$, which perfectly labels all $n$ samples. 
\item If the samples can share either x or y coordinates but not both, can we still learn a decision tree which perfectly labels all $n$ samples? Why or why not?
\end{enumerate}


\paragraph{5) Conjugate Prior (10 points)}
The conjugate priors are very popular in Bayesian data analysis. The formal description of the conjugate priors can be found in Chapter 2.4.2. of Bishop's PRML.
\begin{enumerate}[(a)]
\item Prove that the Gamma distribution with parameters $\alpha$ and $\beta$ is a conjugate prior of the Poisson distribution with parameter $\lambda$.
\item Prove that the Beta distribution with parameters $\alpha$ and $\beta$ is a conjugate prior of the geometric distribution with parameter $p$.

{\bf Hint:} The easiest way to do this is to separate out the "interesting" part of the density from the normalizing constants.
\end{enumerate}


\section{What to Submit}
In each assignment you will submit two things.
\begin{enumerate}
\item {\bf Code:} Your code as a zip file named {\tt library.zip}. {\bf You must submit source code (.java files)}. We will run your code using the exact command lines described above, so make sure it works ahead of time. Remember to submit all of the source code, including what we have provided to you.
\item {\bf Writeup:} Your writeup as a {\bf PDF file} (compiled from latex) containing answers to the analytical questions asked in the assignment. Use the provided tex file for writing your answers.
\end{enumerate}
Make sure you name each of the files exactly as specified (library.zip and writeup.pdf).

To submit your assignment, visit the ``Homework'' section of the website (\href{http://www.cs475.org/}{http://www.cs475.org/}.)

\section{Questions?}
Remember to submit questions about the assignment to the appropriate group on the class discussion board: \href{http://bb.cs475.org/}{http://bb.cs475.org}.

\end{document}

