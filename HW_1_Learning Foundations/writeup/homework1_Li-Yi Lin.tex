\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref} 


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm




\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\pagestyle{myheadings} 
\markboth{Homework 1}{Fall 2015 CS 475 Machine Learning: Homework 1} 


\title{CS 475 Machine Learning: Homework 1\\Learning Foundations\\
\Large{Due: Monday September 14, 2015, 11:59pm}\\
50 Points Total \hspace{1cm} Version 1.0}
\author{Li-Yi Lin / llin34@jhu.edu}
\date{}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\vspace{-.5in}




\section{Analytical (15 Points)}

\paragraph{1 (4 points)} Suppose you are given a biased coin. Specifically, if you flip the coin, it comes up heads with probability $5/6$, and comes up tails with probability $1/6$.  What is the expected number of flips such that the coin comes up tails exactly once.

\noindent
\textbf{Ans:} 
Since we want to know the expected number of flips that there is exactly one tail happen in those flips, the tail must happen in the last flip and we stop tossing the coin. So we can calculate the expected number by 
$$\sum_{x = 1}^{\infty} x \times (5/6)^{x-1} \times 1/6 = 6$$
$x$ is the number of flips.
So the expected number of flips that the coin comes up tails exactly once is 6.


\paragraph{2 (3 points)} Suppose a family have two children. Given the fact that one is a girl, what is the probably that the other child is also a girl?

\noindent
\textbf{Ans:}
Based one the given problem description, we can only assume that the only factor that determines the probability of a baby being girl or boy is the Y-chromosome comes from the baby's father (since women only have X-chromosome). If a Y-chromosome from a man combines with a X-chromosome from a woman, then the baby will be a boy. On the other hand, if a X-chromosome form a man combines with a X-chromosome from a woman, then the baby will be a girl. Then, we further assume that the probabilities of X-chromosome or Y-chromosome comes from a man are both 1/2 and are independent and identically distributed. Therefore, the probability of the other child is also a girl is still 1/2 (i.e. 50\%).


\paragraph{3 (4 points)} Please show that the following loss functions are convex: (a) $\ell (y, \hat{y}) = |y-\hat{y}|$; (b) $\ell (y, \hat{y}) = (y-\hat{y})^2$.

\noindent
\textbf{Ana:}

\noindent
(a) Assume $\hat{y}$, the true value of y, is fixed for each instance. $\forall y_1, y_2 \in R, \forall t \in [0,1]$ if we can prove that $\ell (t \times y_1 + (1-t) \times y_2, \hat{y}) \leq t \times \ell (y_1, \hat{y}) + (1-t) \times \ell (y_2, \hat{y})$,  then the loss function is a convex. We start from the left-hand side of the equation:
$$\ell (t \times y_1 + (1-t) \times y_2, \hat{y}) = | t \times y_1 + (1-t) \times y_2 - \hat{y} |$$
$$= | t \times y_1 + (1-t) \times y_2 - t \times \hat{y} - (1-t) \times \hat{y} |$$
$$\leq |t \times y_1 - t \times \hat{y}| + |(1-t) \times y_2 - (1-t) \times \hat{y_2}|$$
$$= t \times |y_1 - \hat{y}| + (1-t) \times |y_2 - \hat{y}|$$
$$= t \times \ell(y_1, \hat{y}) + (1-t) \times \ell(y_2, \hat{y})$$
Since we have proved that $\ell (t \times y_1 + (1-t) \times y_2, \hat{y}) \leq t \times \ell (y_1, \hat{y}) + (1-t) \times \ell (y_2, \hat{y})$, the loss function is convex.\\

\noindent
(b) To prove the loss function, $\ell (y, \hat{y}) = (y-\hat{y})^2$, is convex, we will need to prove the second derivative of the equation on $y$ is a non-negative function. 
The second derivative of the loss function is 
$$\frac{\partial^2 \ell(y, \hat{y})}{\partial y^2} = 2 > 0$$
Since the second derivative is 2 and it is always large than 0, the loss function is convex.


%\paragraph{4 (2 points)} Given a positive semidefinite symmetric matrix $A\in\mathbb{R}^{n \times n}$, please show $\sum_{i=1}^n A_{ii} = \sum_{i=1}^n\lambda_i$, where $\lambda_1$,...,$\lambda_n$ are eigenvalues of $A$.

%\paragraph{5 (3 points)} Please derive $\nabla\log \det(X)=\left[\frac{\partial\log \det(X)}{\partial X_{ij}}\right]_{i,j=1}^n$, where $X$ is a $n$ by $n$, symmetric, and positive definite matrix.

\paragraph{4 (3 points)} 
Give an example of an optimal hypothesis, a finite hypothesis class that contains the optimal hypothesis, and an infinite class that does not contain the optimal hypothesis.\\
\noindent
\textbf{Ans:} Assume we have a fair dice. The optimal hypothesis of the probability of getting a \textquotedblleft1\textquotedblright from a toss of this dice is 1/6. A finite hypothesis class can be \{1/6, 2/6, 3/6\}. A infinite hypothesis class can be probabilities other than that in the finite hypothesis class, i.e. any number in [0,1] - \{1/6, 2/6, 3/6\}.

\paragraph{5 (1 point)}
How are you comfortable with probability and linear algebra?

\noindent
\textbf{Ans:}
I had learned probability and linear algebra courses when I was a undergraduate student and also reviewed them during this summer. I am comfortable with using them on the Machine Learning course.


\end{document}
